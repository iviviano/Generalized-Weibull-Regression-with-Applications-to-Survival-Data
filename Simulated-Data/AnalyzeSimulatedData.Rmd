# Setup
```{r setup, include=FALSE}
setwd("~/Desktop/R/Generalized-Weibull-Regression-with-Applications-to-Survival-Data/Simulated-Data/")
knitr::opts_chunk$set(echo = FALSE)
options(mc.cores = parallel::detectCores())
library(rstan)
library(rgenoud)
```

# Initialize Dataset
```{r}
sim_data <- read.csv("sim_data.csv")

X <- cbind(sim_data$cont, sim_data$cat)
N <- nrow(X)
P <- ncol(X)

```

# Likelihood Functions
```{r}
exp_weib_ll <- function(params) {
  params <- unlist(params)
  ll <- numeric(N);
  Y <- log(T)
  beta0 <- params[1]; betas <- params[2:(P + 1)]; 
  kappa <- params[P + 2]; theta <- params[P + 3];
  
  w <- (Y - beta0 - X %*% betas) / kappa
  a <- 1 - exp(-exp(w))
  ll <- ifelse(censoring, log(1 - a ^ theta), (theta - 1) * log(a) + w - exp(w))
  sum(ll, na.rm = FALSE) + sum(!censoring) * log(theta / kappa)
}

gen_weib_ll <- function(params) {
  params <- unlist(params)
  ll <- numeric(N);
  Y <- log(T)
  beta0 <- params[1]; betas <- params[2:(P + 1)]; 
  alpha <- params[P + 2]; lambda <- params[P + 3];
  
  w <- (Y - beta0 - X %*% betas) / alpha 
  ll <- ifelse(censoring, 1 / lambda * log(1 - lambda * exp(w)), (1 / lambda - 1) * log(1 - lambda * exp(w)) + w)
  sum(ll, na.rm = FALSE) - sum(!censoring) * log(alpha)
}

weib_ll <- function(params) {
  params <- unlist(params)
  ll <- numeric(N);
  Y <- log(T)
  beta0 <- params[1]; betas <- params[2:(P + 1)]; 
  b <- params[P + 2]; 
  
  w <- (Y - beta0 - X %*% betas) / b
  ll <- ifelse(censoring, -exp(w), w - exp(w))
  sum(ll, na.rm = FALSE) - sum(!censoring) * log(b)
}
```

# Analysis Functions 
```{r}
analyze_results <- function(real_T, generated_T, model) {
  
  model_diagnostics(model)
  
  SSE <- sum((real_T - generated_T) ^ 2)
  print(paste("Sum of squares due to error:", SSE))
  print(sprintf("Average Squared Error: %f", SSE / N))
  AAE <- sum(abs(real_T - generated_T))
  print(paste("Average absolute error:", AAE / N))
  T_ave <- mean(real_T)
  RR <- 1 - SSE / sum((real_T - T_ave) ^ 2) 
  sprintf("Coefficient of determination: %f", RR)
}

model_diagnostics <- function(model) {
  
  if (identical(model, "e")) {
    model_log_likilhood <- exp_weib_ll
    num_model_params <- 2
    param_names <- c("alpha", "lambda")
  }
  else if (identical(model, "g")) {
    model_log_likilhood <- gen_weib_ll
    num_model_params <- 2
    param_names <- c("alpha", "lambda")   
  }
  else {
    model_log_likilhood <- weib_ll
    num_model_params <- 1
    param_names <- c("alpha")
  }
  
  MLE <- genoud(model_log_likilhood, nvars = num_model_params + P + 1, max = TRUE, optim.method = "Nelder-Mead")
  
  modes <- MLE$par
  
  print(sprintf("%s | %s", "Parameter", "MLE"))
  print(sprintf("%-4s%-4d  %+1.3f", "beta", 0, modes[1]))
  for (i in 1:P) 
    print(sprintf("%-4s%-4d  %+1.3f", "beta", i, modes[i + 1]))
  for (i in 1:num_model_params)
    print(sprintf("%-9s  %.3f", param_names[i], modes[1 + i + P]))
  
  max_likelihood <- model_log_likilhood(modes)
  
  AIC(max_likelihood, 1 + P + num_model_params)
  BIC(max_likelihood, 1 + P + num_model_params)
  print(sprintf("Maximum value of log-likelihood is %f", max_likelihood))
}

AIC <- function(max_likelihood, num_params, llog = TRUE) {
  aic <- (ifelse(llog, -2 * max_likelihood, -2 * log(max_likelihood))
          + 2 * num_params)
  print(sprintf("AIC is %f", aic))
  }

BIC <- function(max_likelihood, num_params, llog = TRUE) {
  bic <- (ifelse(llog, -2 * max_likelihood, -2 * log(max_likelihood)) 
           + num_params * log(N))
  print(sprintf("BIC is %f", bic))
}
```

```{r}
###############################################################################
#K-Fold Cross-Validation Function
#Function Parameters:
# -K is the number of partitions for the data to be broken into
# -model is the stan model to be used. Its data should be: 
#       -generate_samples: boolean to generate predictions based off X_new
#       -bayesian: boolean to use nonuniform priors
#       -QR: boolean to use QR reparameterization
#       -norm: boolean if data is normalized (Gelman)
#       -N: the number of observations in the testing data
#       -P: the number of predictor variables
#       -X: the N by P matrix of predictor variables
#       -T: the vector of N testing responses
#       -N_new: the number of responses to be predicted
#       -X_new: the N_new by P matrix of predictors to estimate from
#       -trun: the length N vector telling whether each response was censored
# -QR, norm, T, X, N, P, and censoring are the model data
# -num_model_parameters is the number of non-regression parameters used in model
# -num_chains is the number of chains to run
# -iterations is the number of iterations to run each chain for
# -init_r is the radius for initializing parameters in stan MCMC
#Return Values:
# -A list with the fits and predictions
# -fits is a list of the fits used to predict each partition of the data
# -predictions is a length N vector with predicted values for each response
###############################################################################
K_fold_cross_validate <- function(K, QR, norm, model, T, X, N, P, censoring, num_model_parameters
                             = 2, num_chains = 8, iterations = 5000, init_r = 2) {
    
    generated_Y <- numeric(N);
    generated_T <- numeric(N);
    samples_T <- matrix(0, ncol = N, nrow = 1 + num_chains * iterations / 2)
    fits <- list()
    
    #############################################################################
    # Partition the Data
    #############################################################################
    set.seed(123)
    num_parts <- K
    size_part <- trunc(1 / num_parts * N)
    indices <- sample(1:N, N, replace = FALSE)
    parts <- list()
    for (i in 1:(num_parts - 1)) {
      parts[[i]] <- indices[seq(size_part * (i - 1), size_part * i - 1)]
    }
    parts[[num_parts]] <- indices[seq(size_part * (num_parts - 1), N)]
    
    set.seed(Sys.time())
    
    #############################################################################
    # Run the Model With Each Partition as a Testing Set
    #############################################################################
    for (i in 1:num_parts) {
      print(sprintf("################################ Part %d ################################", i))
      training_parts <- (1:num_parts)[-c(i)]
      testing_indices <- parts[[i]]
      training_indices <- unlist(parts[training_parts])
      X_train <- as.matrix(X[training_indices, ])
      X_test <- as.matrix(X[testing_indices, ])
      Ti <- as.numeric(T[training_indices])
      N_train <- nrow(X_train)
      print(sprintf("N_train is %d", N_train))
      N_test <- nrow(X_test)
      censoring_train <- censoring[training_indices]
      
      fit <- sampling(model, list(generate_samples = TRUE, bayesian = TRUE, QR = QR, norm = norm, N = N_train, X = X_train, T = Ti, P = P, N_new = N_test, X_new = X_test, trun = censoring_train), iter = iterations, chains = num_chains, init_r = init_r)
      
      fits[[i]] <- fit
      print(fit)
      means <- get_posterior_mean(fit, pars = "T_new")
      #samples <- rstan::extract(fit, pars = "T_new")
      
      generated_T[testing_indices] <- means[, num_chains + 1]
      
      #samples_T[, testing_indices] <- rbind(generated_T[testing_indices], matrix(unlist(samples), byrow = FALSE, ncol = N_test))
    }
    return(list(fits = fits, generated_T = generated_T))#, samples_T = samples_T))
  }
```

```{r}
###############################################################################
#Leave-one-out Cross-Validation Function
#Function Parameters:
# -model is the stan model to be used. Its data should be: 
#       -generate_samples: boolean to generate predictions based off X_new
#       -bayesian: boolean to use nonuniform priors
#       -QR: boolean to use QR reparameterization
#       -norm: boolean if data is normalized (Gelman)
#       -N: the number of observations in the testing data
#       -P: the number of predictor variables
#       -X: the N by P matrix of predictor variables
#       -T: the vector of N testing responses
#       -N_new: the number of responses to be predicted
#       -X_new: the N_new by P matrix of predictors to estimate from
#       -trun: the length N vector telling whether each response was censored
# -QR, norm, T, X, N, P, and censoring are the model data
# -num_model_parameters is the number of non-regression parameters used in model
# -num_chains is the number of chains to run
# -iterations is the number of iterations to run each chain for
# -init_r is the radius for initializing parameters in stan MCMC
#Return Values:
# -A list with the fits and predictions
# -fits is a list of the fits used to predict each partition of the data
# -predictions is a length N vector with predicted values for each response
###############################################################################
LOO_cross_validate <- function(QR, norm, model, T, X, N, P, censoring, num_model_parameters
                             = 2, num_chains = 8, iterations = 5000, init_r = 2) {
    
    generated_T <- numeric(N);
    fits <- list()
    
    #############################################################################
    # Run the Model Leaving one Data Point out of the Training Set Each Iteration
    #############################################################################
    for (i in 1:N) {
      print(sprintf("################################ Part %d ################################", i))
      X_train <- as.matrix(X[-i, ])
      X_test <- matrix((X[i, ]), nrow = 1, ncol = 2)
      Ti <- as.numeric(T[-i])
      N_train <- N - 1
      #print(sprintf("N_train is %d", N_train))
      N_test <- 1
      censoring_train <- censoring[-i]
      
      fit <- sampling(model, list(generate_samples = TRUE, bayesian = TRUE, QR = QR, norm = norm, N = N_train, X = X_train, T = Ti, P = P, N_new = N_test, X_new = X_test, trun = censoring_train), iter = iterations, chains = num_chains, init_r = init_r)
      
      fits[[i]] <- fit
      print(fit)
      means <- get_posterior_mean(fit, pars = "T_new")
      #samples <- rstan::extract(fit, pars = "T_new")
      
      generated_T[i] <- means[num_chains + 1]
      
      #samples_T[, testing_indices] <- rbind(generated_T[testing_indices], matrix(unlist(samples), byrow = FALSE, ncol = N_test))
    }
    return(list(fits = fits, generated_T = generated_T))#, samples_T = samples_T))
  }
```

# Running the analysis
```{r}
t_col <- 4

T <- sim_data[, t_col]
censoring <- sim_data[, t_col + 1]
real_T <- sim_data[, t_col + 2]
model <- stan_model("../Shiny/general_regression.stan")
results <- K_fold_cross_validate(5, FALSE, TRUE, model, T, X, N, P, censoring)
# results <- LOO_cross_validate(FALSE, TRUE, model, T, X, N, P, censoring)
generated_T <- results$generated_T
analyze_results(real_T, generated_T, model = "g")
cat(sprintf("These numbers are for %s", names(sim_data)[t_col]))
```

